{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for IDS Models\\n,\n",
    "    \n",
    "    This notebook demonstrates data preprocessing for Intrusion Detection System (IDS) models using the UNSW-NB15 dataset. The pipeline includes handling missing values, encoding categorical features, applying feature scaling (with optional log transformation for skewed features), and ensuring consistent label encoding. Saving the preprocessing pipeline guarantees that the same transformations are applied during both training and inference.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, FunctionTransformer, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Load the UNSW-NB15 training and test datasets\n",
    "# Replace these filenames with the correct paths to your dataset files\n",
    "df_train = pd.read_csv('D:\\\\Optimization-Research\\\\Hybrid_Whale_RIME_IDS_Project\\\\UNSW_NB15\\\\data\\\\row\\\\UNSW_NB15_training-set.csv')\n",
    "df_test = pd.read_csv('D:\\\\Optimization-Research\\\\Hybrid_Whale_RIME_IDS_Project\\\\UNSW_NB15\\\\data\\\\row\\\\UNSW_NB15_testing-set.csv')\n",
    "\n",
    "# Display the first few rows of the training dataset\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration and Handling Missing Values\n",
    "\n",
    "Examine the datasets for missing values. In IDS datasets like UNSW-NB15, missing values might be rare but should be handled appropriately to ensure robust preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display missing values per column in the training dataset\n",
    "print(\"Missing values in training dataset:\")\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "# Optionally, check missing values in the test dataset\n",
    "print(\"\\nMissing values in test dataset:\")\n",
    "print(df_test.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify Numeric and Categorical Features\n",
    "\n",
    "For effective preprocessing, we separate numeric and categorical features. We also drop high-cardinality identifiers (e.g., source/destination IP addresses) from both datasets to ensure consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop high-cardinality columns\n",
    "def drop_high_cardinality(df):\n",
    "    columns_to_drop = ['srcip', 'dstip'] if set(['srcip', 'dstip']).issubset(df.columns) else []\n",
    "    return df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Apply the function to both training and test datasets\n",
    "df_train = drop_high_cardinality(df_train)\n",
    "df_test = drop_high_cardinality(df_test)\n",
    "\n",
    "# Identify numeric columns from the training dataset\n",
    "numeric_cols = df_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Define the target column (adjust if needed, e.g., 'attack' or 'label')\n",
    "target_column = 'label'\n",
    "\n",
    "# Remove the target column from numeric features if it is present\n",
    "if target_column in numeric_cols:\n",
    "    numeric_cols.remove(target_column)\n",
    "\n",
    "# Identify categorical columns from the training dataset\n",
    "categorical_cols = df_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "print(\"Categorical columns:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Preprocessing Pipelines\n",
    "\n",
    "### 3.1 Numeric Pipeline\n",
    "\n",
    "The numeric pipeline includes:\n",
    "- **Imputation:** Filling missing numeric values using the mean strategy.\n",
    "- **Log Transformation:** An optional step to transform skewed features (e.g., `dbytes`) using a log transform to reduce the influence of outliers.\n",
    "- **Scaling:** Normalizing features using Min-Max scaling to bring them into the [0, 1] range.\n",
    "\n",
    "The log transformation is applied selectively. Adjust the transformation based on your domain knowledge and data distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for log transformation using log1p (to handle zero values)\n",
    "def log_transform(values):\n",
    "    return np.log1p(values)\n",
    "\n",
    "# Function to selectively apply log transformation on the column corresponding to 'dbytes'\n",
    "# Here, X is expected to be a NumPy array and feature_names is the list of column names in order.\n",
    "def select_and_log_transform(X, feature_names):\n",
    "    X_transformed = X.copy()\n",
    "    if 'dbytes' in feature_names:\n",
    "        idx = feature_names.index('dbytes')\n",
    "        X_transformed[:, idx] = log_transform(X_transformed[:, idx])\n",
    "    return X_transformed\n",
    "\n",
    "# Define a top-level function to apply the log transformation using the numeric_cols list\n",
    "def apply_log_transform(X):\n",
    "    # 'numeric_cols' should be defined in the global scope (from Code Block 3)\n",
    "    return select_and_log_transform(X, numeric_cols)\n",
    "\n",
    "# Create the numeric pipeline using the named function 'apply_log_transform'\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('log_transform', FunctionTransformer(apply_log_transform, validate=False)),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Categorical Pipeline\n",
    "\n",
    "For categorical features, the pipeline includes:\n",
    "- **Imputation:** Filling missing values with a constant value (e.g., 'unknown').\n",
    "- **Encoding:** Converting categorical variables to numeric using one-hot encoding. This ensures that the model does not assume any ordinal relationship among categories.\n",
    "\n",
    "For some models (such as tree-based algorithms), label encoding may also be appropriate. In this example, we use one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining Pipelines with ColumnTransformer\n",
    "\n",
    "We use a `ColumnTransformer` to combine the numeric and categorical pipelines so that each set of columns receives the appropriate transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_pipeline, numeric_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparing the Target Label\n",
    "\n",
    "The target label in UNSW-NB15 can be binary (e.g., 0 for normal and 1 for attack) or multi-class. This cell checks whether the target is already numeric. If not, label encoding is applied to the training dataset, and the same transformation is used on the test dataset (if available). This ensures consistency in how the labels are represented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the target label in the training dataset is not numeric, apply label encoding\n",
    "if df_train[target_column].dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    df_train[target_column] = le.fit_transform(df_train[target_column])\n",
    "    \n",
    "    # If the test dataset contains the target column and it is non-numeric, apply the same encoding\n",
    "    if target_column in df_test.columns and df_test[target_column].dtype == 'object':\n",
    "        df_test[target_column] = le.transform(df_test[target_column])\n",
    "    \n",
    "    # Save the label encoder for use during inference\n",
    "    joblib.dump(le, 'label_encoder.joblib')\n",
    "    print(\"Label encoding applied on training dataset.\")\n",
    "else:\n",
    "    print(\"Target column is already numeric in training dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Applying the Preprocessing Pipeline\n",
    "\n",
    "With the preprocessing pipeline defined, we now apply it to the training and test datasets. The pipeline is fitted on the training data, and the same transformations are applied to the test data to ensure consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target label for training and test datasets\n",
    "X_train = df_train.drop(columns=[target_column])\n",
    "y_train = df_train[target_column]\n",
    "\n",
    "# For the test dataset, if the target column exists, separate it; otherwise, work only with features\n",
    "X_test = df_test.drop(columns=[target_column]) if target_column in df_test.columns else df_test.copy()\n",
    "y_test = df_test[target_column] if target_column in df_test.columns else None\n",
    "\n",
    "# Fit the preprocessor on the training data and transform the training features\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to the test features\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Preprocessing complete for both training and test datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving the Preprocessing Pipeline\n",
    "\n",
    "To ensure consistency during future inference, save the fitted preprocessing pipeline. This step preserves the exact transformations applied to the training data so that they can be reused on new incoming data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(preprocessor, 'preprocessing_pipeline.joblib')\n",
    "print(\"Preprocessing pipeline saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided a step-by-step guide for preprocessing the UNSW-NB15 dataset for IDS models using two separate datasets (training and test). The process included:\n",
    "\n",
    "- Handling missing values via imputation for both numeric and categorical data.\n",
    "- Encoding categorical features with one-hot encoding (with an option for label encoding).\n",
    "- Applying feature scaling (with an optional log transformation for skewed numeric features).\n",
    "- Ensuring the target label is in the correct numeric format.\n",
    "- Saving the preprocessing pipeline to avoid training-serving skew during future inference.\n",
    "\n",
    "This systematic approach is crucial for building reliable and consistent IDS models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
